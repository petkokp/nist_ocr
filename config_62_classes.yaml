# Configuration for full 62-class NIST OCR training
# Classes: A-Z (uppercase), a-z (lowercase), 0-9 (digits)

# Dataset configuration - scaled for 62 classes
dataset:
  root: "."
  train_partitions:
    - train
  test_partitions:
    - hsf_7
    - hsf_4
    - hsf_0
  train_limit: null  
  test_limit: null     
  image_size: 32

#LBP + Linear SVM             
lbp:
  radius: 4 # BASELINE IS 2, for 62c config trying higher
  n_points: null
  method: "uniform"
  class_weight: "balanced"
  max_iter: 3000 

# HOG + Linear SVM - optimized for large datasets
hog:
  orientations: 9
  pixels_per_cell: [8, 8]
  cells_per_block: [2, 2]
  class_weight: "balanced"
  max_iter: 5000          # Increase for large datasets (default 1000)

# Zernike Moments + SVM - tuned for more classes
zernike:
  radius: 30
  degree: 15            # Higher degree for more discriminative features
  kernel: "rbf"
  C: 10.0
  gamma: "scale"

# Projection Histograms + kNN - tuned for more classes
projection:
  n_neighbors: 7        # More neighbors for stability with 62 classes
  weights: "distance"
  metric: "euclidean"
  normalize: true

# Preprocessing - enabled for better feature extraction
preprocessing:
  enabled: true
  adaptive_threshold: true
  threshold_method: "otsu"
  morphology: true
  morph_operation: "opening"
  morph_kernel_size: 3
  normalize: true
  deskew: true

# Data augmentation - important for 62 classes
augmentation:
  rotation_range: 5 #10        # Slightly less rotation to preserve letter identity
  elastic_alpha: 0 #20         # Reduced elastic distortion
  elastic_sigma: 4
  translation_range: 0.05
  scale_range: [0.95, 1.05]
  noise_std: 0.02 #0.03
  apply_prob: 0.3 #0.5

# CNN configuration - scaled for 62 classes (runs both CNN and CNN + Aug automatically)
cnn:
  epochs: 30
  batch_size: 256 #64
  learning_rate: 0.0005 #0.001
  optimizer: "adamw"  # AdamW with weight decay helps generalization
  weight_decay: 0.0005 #previous NO
  use_scheduler: true
  early_stopping_patience: 5 #7
  label_smoothing: 0.1 # previous NO

# ResNet configuration - scaled for 62 classes
resnet:
  epochs: 30
  batch_size: 256
  learning_rate: 0.0005
  optimizer: "adamw"
  weight_decay: 0.0005
  use_scheduler: true
  early_stopping_patience: 5
  label_smoothing: 0.1

# Experiment configuration
experiment:
  skip_learning_curves: true  # CRITICAL: Skip to save RAM (trains 6x fewer times)
  skip_lbp: false
  skip_hog: false
  skip_cnn: false
  skip_resnet: false
  skip_zernike: false
  skip_projection: false
  checkpoint_dir: "checkpoints"
  results_dir: "results_62_classes"
  save_predictions: true

# Visualization
visualization:
  confusion_matrix: true
  per_class_metrics: true
  misclassifications: true
  max_misclassifications: 30
  training_history: true
  comparison_plot: true
